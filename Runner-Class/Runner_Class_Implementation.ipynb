{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPazB+x3M1ShynR4X1b0iYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saim-Hassan786/Learn-Agentic-AI/blob/main/Runner-Class/Runner_Class_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner\n",
        "**Runner is very crucial and imperative feature of OpenAI Agents SDK , it acts as an Agent loop that is responsible for orchestrating ,running and implementing all the Agents,Tools,Guardrails,Handoffs,LLM configuration etc until the desired ouput in relative to the user query is achieved.It can be regarded as a manager loop that keeps running with all the sdk code logic until it achieves the desired output or encounter an error.**"
      ],
      "metadata": {
        "id": "TCoBwQ0rzjO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhrHYsAdzNny",
        "outputId": "089f5f2c-e63e-473e-a346-c090c016b874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the SDK\n",
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running event loop\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "J9rYqXSSzYEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre requisites SetUp\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY= userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from agents import set_default_openai_api,set_default_openai_client,set_tracing_disabled\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "set_default_openai_client(external_client)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "set_tracing_disabled(True)"
      ],
      "metadata": {
        "id": "JaFph2tAzaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner Simple Example"
      ],
      "metadata": {
        "id": "E2pA8Rsi1Q8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    starting_agent = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        "),\n",
        "    input = \"what is the capital of sweden\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WnY4PD4GzgLt",
        "outputId": "709ebc5f-9428-4f0f-edae-96d31309c861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Sweden is **Stockholm**.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner Class Methods\n",
        "A runner class has 3 cls methods :\n",
        "\n",
        "1. **Runner.run()** :: asynchronous\n",
        "2. **Runner.run_sync()** :: synchronous block over async Runner.run\n",
        "3. **Runner.run_streamed()** :: async generator of stream events for Streming"
      ],
      "metadata": {
        "id": "1Bkyavao1V4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner.run()\n",
        "Runner.run is an async method that has following parameters and return a **RunResult** Object\n",
        "\n",
        "1. **starting_agent** :: positional argument\n",
        "2. **input** :: positional argument\n",
        "3. **context** :: keyword argument\n",
        "4. **hooks** :: RunHooks :: keyword argument\n",
        "5. **run_config** :: RunConfig :: keyword argument\n",
        "6. **max_turns** :: DEFAULT_MAX_TURNS\n",
        "6. **previous_response_id** :: For Responses APIs"
      ],
      "metadata": {
        "id": "52lYPUZq3LfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,function_tool\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Input(BaseModel):\n",
        "    name : str\n",
        "    age : int\n",
        "\n",
        "@function_tool\n",
        "def add(a:int,b:int):\n",
        "    return a+b\n",
        "\n",
        "agent_1 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [add],\n",
        "    instructions=\"HelpFul Assistant\",\n",
        "    tool_use_behavior=\"stop_on_first_tool\"\n",
        "    # tool_use_behavior = \"run_llm_again\"\n",
        ")\n",
        "\n",
        "result_1 = await Runner.run(\n",
        "    starting_agent = agent_1,\n",
        "    input = \"what is the answer of adding 4 and 5\",\n",
        "    max_turns = 1,\n",
        "    # max_turn = 2  It will generate an error as the LLM asks for tool call and toll result when goes back to LLM the Max Turns will be exceeded and it errors\n",
        ")\n",
        "print(result_1.final_output)\n",
        "\n",
        "# 2nd case\n",
        "agent_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        ")\n",
        "\n",
        "result_2 = await Runner.run(\n",
        "    starting_agent = agent_2,\n",
        "    input = \"what is the capital of france\",\n",
        "    max_turns = 1, # As LLM respond with the ouput that is considered as the FinalOutput so the loop terminates with answer and result is achieved in 1 max turn without error\n",
        ")\n",
        "print(result_2.final_output)\n",
        "\n",
        "# 3rd case\n",
        "agent_3 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [add],\n",
        "    instructions=\"General purpose Assistant that answer the user queries and use tool when required\",\n",
        ")\n",
        "\n",
        "result_3 = await Runner.run(\n",
        "    starting_agent = agent_3,\n",
        "    input = \"what is my name\",\n",
        "    max_turns = 7,\n",
        "    context = Input(name = \"Saim\",age = 25) #context is only available for Agent,Tools etc internally ,never given to LLM\n",
        ")\n",
        "print(result_3.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqYyvI8V9F5Y",
        "outputId": "2a2de788-3b75-49f7-b707-cf46ea246ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "The capital of France is **Paris**.\n",
            "I do not know your name. I am a large language model, not a personal assistant.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OverAll Working Of Runner.run()\n",
        "\n",
        "1. When we start an agent loop with Runner.run(), according to new experimental documentation, DEFAULT_AGENT_RUNNER class is initialized and then all the parameters are called with the **run** method in it.\n",
        "2. In run() method, first all th parameters are retrieved and 2 parameters are initialized if not given **run_config** and **hooks**.\n",
        "3. Then the next preparations are as follows:\n",
        "\n",
        "    - **tool-use_tracker** is initialized\n",
        "    - **should_start_agent_hook** is initialized to True\n",
        "    - **Tracing** is enabled from all the parameters of run_config\n",
        "    - **current_span** is initialized\n",
        "    - **generated_items** list is [ ] initialized\n",
        "    - **model_response** list is [ ] initialized\n",
        "    - **context** is wrapped in RuncontextWrapper\n",
        "    - **input_quardrails** list is [ ] initialized\n",
        "    - **output_quardrails** list is [ ] initialized\n",
        "    - **original_input** deepcopy is initialized\n",
        "    - **current_turn** is set to 0\n",
        "    - **current_agent** is set to **starting_agent** passed in Runner.\n",
        "\n",
        "4. After all the above preparations a **current_span** is initialzed if it is None with all the **handoffs**, **output_schema**, **tools** and **name** of the current_agent in a **while** loop\n",
        "5. Now if the **current_turn** is greater than **max_turns** defined in Runner.run() then the Error is spaned in current_span and the **MaxTurnsExceeded** error is raised and the loop terminates\n",
        "6. If **current_turn** is equal to 1 then all the **input_guardrails** are retrived from the current_agent and run_config are run with the original input.If no exception is raised then the **run_single_turn** func with the above retreived parameters are run.\n",
        "7. An **else** block is also set to execute the **run_single_turn** func if the current turn is more that 1 and thi is run without **input_gaurdrails**.\n",
        "8. After the 1st turn, all the generated_items,model_responses and original_items are appended to the **turn_result** variable that is initialized for running **run_single_turn**\n",
        "8. Now after the 1st turn, LLM decides what will be the **Next_Step**\n",
        "  \n",
        "   - If next_step is **NextStepFinalOutput** then all the **output_gaurdrails** are retrived from the current_agent and run_config and are run with the **output** and the if no error then loop terminates with **RunResult** containing final output.\n",
        "   - If next_step is **NextStepHandOff** then the **current_agent** is changed with the **new_agent** name, the **current_span** is finished, **current_span** is set to None and **should_start_agent_hook** is also set to True for the new_agent to take control.\n",
        "   - If next_step is **NextStepRunAgain** then just use **pass** for the loop to continue the next turn.\n",
        "   - If there is any next_step that is undefined then **AgentExceptionError** is rasied with all the error details\n",
        "10. At last , is there is any **current_span** running , finishes it off gracefully.\n",
        "\n"
      ],
      "metadata": {
        "id": "nbRXv7whMC90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runner.run() example\n",
        "from agents import Agent,Runner\n",
        "\n",
        "agent_with_run = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        ")\n",
        "result_with_run = await Runner.run(\n",
        "    starting_agent = agent_with_run,\n",
        "    input = \"who is Geoffery Hintman\"\n",
        ")\n",
        "print(result_with_run.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KIyVDqiaxHT",
        "outputId": "5854be4d-afc4-45c6-e4b8-550f495d632b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Geoffrey Hinton** is a highly renowned British-Canadian cognitive psychologist and computer scientist, widely considered one of the \"Godfathers of AI\" or the \"Godfather of Deep Learning.\"\n",
            "\n",
            "Here's why he's so important:\n",
            "\n",
            "1.  **Pioneering Deep Learning:** Hinton is a central figure in the field of artificial intelligence, particularly for his groundbreaking work on artificial neural networks and deep learning. His research laid much of the theoretical and practical foundation for the deep learning revolution we're seeing today.\n",
            "\n",
            "2.  **Backpropagation Algorithm:** One of his most significant contributions was his work on the **backpropagation algorithm** in the 1980s. This algorithm was crucial for efficiently training multi-layer neural networks, making them practical for real-world applications.\n",
            "\n",
            "3.  **Neural Network Research:** He dedicated decades to exploring how neural networks learn, represent information, and perform complex tasks like speech recognition and computer vision, long before these applications became mainstream. He also developed concepts like Boltzmann machines and Hinton diagrams.\n",
            "\n",
            "4.  **Academic and Industry Influence:**\n",
            "    *   He spent many years as a distinguished professor at the **University of Toronto**.\n",
            "    *   Later, he became a key researcher at **Google** (joining with his team to establish part of the Google Brain team), where he played a pivotal role in the company's AI advancements.\n",
            "    *   In May 2023, he notably resigned from Google to speak more freely about the potential risks of AI.\n",
            "\n",
            "5.  **Turing Award Recipient:** In recognition of his immense contributions, he was awarded the **ACM A.M. Turing Award** (often called the \"Nobel Prize of Computing\") in 2018, sharing it with Yann LeCun and Yoshua Bengio. They received the award for their \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\"\n",
            "\n",
            "In essence, Geoffrey Hinton's relentless pursuit of understanding how brains learn and translate that into computational models has profoundly influenced the development of modern AI technologies like voice assistants, image recognition systems, machine translation, and countless other applications that are now an integral part of our daily lives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OverAll Working Of Runner.run_sync\n",
        "\n",
        "1. Similary, When we start an agent loop with Runner.run_sync(), according to new experimental documentation, DEFAULT_AGENT_RUNNER class is initialized and then all the parameters are called with the **run_sync** method in it.\n",
        "2. It in the same way retrives all the parameters passed to it and others not passsed are set to None.\n",
        "3. It is just a synchronous block over the asynchronous **run()** method.\n",
        "4. So what it simple does here is it gets a new event_loop and execute the **run()** method with all the parameters retreived passed in it and **block its execution until completion** to make it a synchronous function at the end.\n",
        "5. So async **run()** executes in it until completion and at the end the **RunResult** object is returned with teh final output."
      ],
      "metadata": {
        "id": "gwBK0spGbc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "agent_with_run_sync = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        ")\n",
        "\n",
        "# import nest_asyncio  In Jupyter nootbooks as the event loop is already running so creating a new event loop is not permitted by jupyter to run our run_sync method we have to run this code that enables the nested loop execution.\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "result_with_run_sync = Runner.run_sync(\n",
        "    starting_agent = agent_with_run_sync,\n",
        "    input = \"who is the founder of OpenAI\"\n",
        ")\n",
        "print(result_with_run_sync.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYg8Rj8BsAG",
        "outputId": "32089cce-4de3-452d-efc7-9275020cdcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI was founded by a group of individuals, so there isn't one single \"founder\" in the traditional sense. It was established as a non-profit research company in **December 2015** by:\n",
            "\n",
            "*   **Sam Altman**\n",
            "*   **Elon Musk**\n",
            "*   **Ilya Sutskever**\n",
            "*   **Greg Brockman**\n",
            "*   **Wojciech Zaremba**\n",
            "*   **John Schulman**\n",
            "\n",
            "Elon Musk was an initial co-chairman and significant funder, though he later stepped down from the board in 2018. Sam Altman has served as the CEO for most of its operational history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OverAll Working Of Runner.run_streamed()\n",
        "\n",
        "1. Similary, When we start an agent loop with Runner.run_streamed(), according to new experimental documentation, DEFAULT_AGENT_RUNNER class is initialized and then all the parameters are called with the run_streamed method in it.\n",
        "2. All the parameters are retrieved and passed in run_streamed(), then a variable named **streamed_result** is initialzied for a **RunResultStreaming** class and filled with paramters like, current_agent, current_span, is_complete, input_guardrails_list,output_guardrails_list,hooks , run_config , started trace, wrapping context in RunContextWrapper etc we have done it to return the RunResultStreaming object immediately and we will keep updating with the real values created by the **start_streaming** function below.\n",
        "3. Then a couroutine is created and the function named **start_streaming** is run with all the actual parameters passed in **run_streamed** with an additional **streamed_result** passed in it as well that is the instance of **RunResultStreaming**.\n",
        "4. In **start_streaming** it starts with all the original parameters passed in **run_streamed** with an additional **RunResultStreaming** object for updating continously in real time. In it fist all the parameters like **current_span**, **tool_use_tracker**, **should_start_agent_hook** , **current_turn** etc and an **AgentUpdatedStreamedEvent** is sent to **stream_event_queue** to stream.\n",
        "5. A while loop is started and if **streamed_result.is_complete** is Ture then break the loop and streaming ,then a **current_span** is created with all the tools, handoffs,output_schema and current_agent name and the current_turn counter is set to **+= 1** after each turn.\n",
        "6. Similarly , if **current_turn** is greature than **max_turns** then the **QueueSentinel** is rasied in **streamed_result** event queue and the loop is broken.\n",
        "7. If the **current_turn** is **== 1** then run the **run_input_guardrails_with_queue** on all the guardrails from the current_agent and the run_config guardrails as well and if no error run the **run_single_turn_streamed** with all the parameters and **should_start_agent_hook** is set to False after first turn.\n",
        "8. And an **else** bolck is also set there if the current_turn is more than 1 to run **run_single_turn-streamed** but this time without **run_input_guardrails_with_queue** and the **raw_responses**, **new_items**, **original_input** are updated in the streamed_result continously for streaming.\n",
        "9. Then the LLM decide what will be the next_step:\n",
        "\n",
        "   - If next_step is **NextStepFinalOutput** then all the **output_gaurdrails** are retrived from the current_agent and run_config and are run with the **output** and the if no error then all the events are updated in the streamed_result and **QueueSentinel** is sent to signal no upcoming_streames and the **is_complete** is set to true that terminates the loop..\n",
        "   - If next_step is **NextStepHandOff** then the **current_agent** is changed with the **new_agent** name,**AgentUpdatedStreamEvent** is sent to signal new agent taking control, the **current_span** is finished, **current_span** is set to None and **should_start_agent_hook** is also set to True .\n",
        "   - If next_step is **NextStepRunAgain** then just use **pass** for the loop to continue the next turn.\n",
        "   - If there is any next_step that is undefined then **AgentException** **is_complete** is set to True that breakes the loop, **QueueSentinel** is sent to signal stopping stream events and the it is raised containing all the Error details\n",
        "\n",
        "10. At last , is there is any **current_span** running , finishes it off gracefully."
      ],
      "metadata": {
        "id": "Rp14eOAOnyBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "agent_with_run_streamed = Agent(\n",
        "    name=\"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "result_with_run_streamed = Runner.run_streamed(\n",
        "    starting_agent = agent_with_run_streamed,\n",
        "    input = \"who is the founder of Anthropic\"\n",
        ")\n",
        "async for event in result_with_run_streamed.stream_events():\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlaJZjW4d_Qj",
        "outputId": "acfa8b84-5c32-4970-8fd1-e6872d860277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthropic doesn't have a single \"founder\" in the traditional sense, as it was started by a group of former OpenAI employees.\n",
            "\n",
            "However, the most prominent figures and the core of the founding team are:\n",
            "\n",
            "*   **Dario Amodei** (CEO)\n",
            "*   **Daniela Amodei** (President)\n",
            "\n",
            "They, along with many other researchers, left OpenAI primarily due to disagreements over the direction and safety focus of AI development, and then co-founded Anthropic in 2021 to focus on large language models and AI safety research in a more controlled and responsible way."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunResult\n",
        "**The Return Objects from Runner.run() and Runner.run_sync()**"
      ],
      "metadata": {
        "id": "RH7FHgdOQRsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "\n",
        "agent_with_run_result = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        ")\n",
        "result_with_run_result = await Runner.run(\n",
        "    starting_agent = agent_with_run_result,\n",
        "    input = \"what is the capital of Germany\"\n",
        ")"
      ],
      "metadata": {
        "id": "zEkiL_PfQVsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_with_run_result)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.final_output)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.new_items)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.raw_responses)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.input_guardrail_results)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.output_guardrail_results)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.last_agent)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.last_response_id)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result.context_wrapper)\n",
        "print(\"==========\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhod4CpWRd-y",
        "outputId": "2c2828ee-ad15-40a1-95f5-7b60f0c24774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Assistant Agent\", ...)\n",
            "- Final output (str):\n",
            "    The capital of Germany is **Berlin**.\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n",
            "========================================================================================================================================================================================================\n",
            "The capital of Germany is **Berlin**.\n",
            "========================================================================================================================================================================================================\n",
            "[MessageOutputItem(agent=Agent(name='Assistant Agent', instructions='General purpose Assistant that answer the user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The capital of Germany is **Berlin**.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n",
            "========================================================================================================================================================================================================\n",
            "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The capital of Germany is **Berlin**.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=16, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=8, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=47), response_id=None)]\n",
            "========================================================================================================================================================================================================\n",
            "[]\n",
            "========================================================================================================================================================================================================\n",
            "[]\n",
            "========================================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='General purpose Assistant that answer the user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "========================================================================================================================================================================================================\n",
            "None\n",
            "========================================================================================================================================================================================================\n",
            "RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=16, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=8, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=47))\n",
            "========================================================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunResultStreaming\n",
        "**The Return Object from Runner.run_streamed()**"
      ],
      "metadata": {
        "id": "sPwDVnyTe15o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "agent_with_run_result_streaming = Agent(\n",
        "    name=\"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "result_with_run_result_streaming = Runner.run_streamed(\n",
        "    starting_agent = agent_with_run_streamed,\n",
        "    input = \"who is the prime minister of pakistan\"\n",
        ")\n",
        "async for event in result_with_run_result_streaming.stream_events():\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oie0f5bFRkeL",
        "outputId": "1dbf6a3e-a83b-4104-8f41-9001157c614a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current Prime Minister of Pakistan is **Shehbaz Sharif**.\n",
            "\n",
            "He assumed office for his second term on March 4, 2024, following the general elections held in February 2024."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_with_run_result_streaming)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.final_output)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.new_items)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.raw_responses)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.input_guardrail_results)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.output_guardrail_results)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.last_agent)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.last_response_id)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.context_wrapper)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.is_complete)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.current_turn)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.max_turns)\n",
        "print(\"==========\"*20)\n",
        "print(result_with_run_result_streaming.current_agent)\n",
        "print(\"==========\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fff2OBfvfX7w",
        "outputId": "5e129c66-6f61-4556-8214-6cbb9e7bbcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Assistant Agent\", ...)\n",
            "- Current turn: 1\n",
            "- Max turns: 10\n",
            "- Is complete: True\n",
            "- Final output (str):\n",
            "    The current Prime Minister of Pakistan is **Shehbaz Sharif**.\n",
            "    \n",
            "    He assumed office for his second term on March 4, 2024, following the general elections held in February 2024.\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details)\n",
            "========================================================================================================================================================================================================\n",
            "The current Prime Minister of Pakistan is **Shehbaz Sharif**.\n",
            "\n",
            "He assumed office for his second term on March 4, 2024, following the general elections held in February 2024.\n",
            "========================================================================================================================================================================================================\n",
            "[MessageOutputItem(agent=Agent(name='Assistant Agent', instructions='General purpose Assistant that answer the user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The current Prime Minister of Pakistan is **Shehbaz Sharif**.\\n\\nHe assumed office for his second term on March 4, 2024, following the general elections held in February 2024.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n",
            "========================================================================================================================================================================================================\n",
            "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The current Prime Minister of Pakistan is **Shehbaz Sharif**.\\n\\nHe assumed office for his second term on March 4, 2024, following the general elections held in February 2024.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0), response_id='__fake_id__')]\n",
            "========================================================================================================================================================================================================\n",
            "[]\n",
            "========================================================================================================================================================================================================\n",
            "[]\n",
            "========================================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='General purpose Assistant that answer the user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "========================================================================================================================================================================================================\n",
            "__fake_id__\n",
            "========================================================================================================================================================================================================\n",
            "RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "========================================================================================================================================================================================================\n",
            "True\n",
            "========================================================================================================================================================================================================\n",
            "1\n",
            "========================================================================================================================================================================================================\n",
            "10\n",
            "========================================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='General purpose Assistant that answer the user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "========================================================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunConfig\n",
        "**The configuration settings in the run_config parameter of Runner that will override the settings in model_settings parameter of Agent class.**"
      ],
      "metadata": {
        "id": "POQ25KTyX7wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is the Example Of :\n",
        "\n",
        "**1. model**\n",
        "\n",
        "**2. model_provider**\n",
        "\n",
        "**3. model_settings**"
      ],
      "metadata": {
        "id": "Mls-Fv0L8nlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,RunConfig,ModelSettings,ModelProvider,OpenAIChatCompletionsModel,Model\n",
        "\n",
        "agent_with_run_config = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        ")\n",
        "\n",
        "class CustomModelProvider(ModelProvider):\n",
        "  def get_model(self,model_name)->Model:\n",
        "    return OpenAIChatCompletionsModel(model = model_name, openai_client=external_client)\n",
        "\n",
        "result_with_run_config = await Runner.run(\n",
        "    starting_agent = agent_with_run_config,\n",
        "    input = \"Who is the founder of Python language and why it is named so\",\n",
        "    run_config= RunConfig(\n",
        "        model = \"gemini-2.5-flash\",\n",
        "        model_provider = CustomModelProvider(),\n",
        "        model_settings=ModelSettings(\n",
        "            temperature = 0.1,\n",
        "            max_tokens = 1000\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "mVryh2SnX-g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_with_run_config.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdf-So_bZQys",
        "outputId": "5a7853f2-777c-410f-ef77-d6e9c54d5068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The founder of the Python language is **Guido van Rossum**.\n",
            "\n",
            "He is a Dutch programmer who began working on Python in the late 1980s, specifically around December 1989, at Centrum Wiskunde & Informatica (CWI) in the Netherlands. He served as the project's Benevolent Dictator For Life (BDFL) until July 2018, meaning he oversaw the Python development process and made final decisions.\n",
            "\n",
            "As for **why it is named Python**:\n",
            "\n",
            "It's not named after the snake, as many might assume! Guido van Rossum was a big fan of the British comedy sketch show **\"Monty Python's Flying Circus\"**.\n",
            "\n",
            "He wanted a name that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# handoff_input_filter"
      ],
      "metadata": {
        "id": "FLlA6OjO8fLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,RunConfig,ModelSettings,ModelProvider,OpenAIChatCompletionsModel,Model,HandoffInputData\n",
        "\n",
        "agent_with_run_config_handoff = Agent(\n",
        "    name = \"Essay Writer Agent\",\n",
        "    instructions=\"Write a 50 words essay for the topic given to you\",\n",
        ")\n",
        "\n",
        "agent_with_run_config_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    handoffs=[agent_with_run_config_handoff]\n",
        ")\n",
        "\n",
        "def my_handoff_filter(data: HandoffInputData) -> HandoffInputData:\n",
        "    print(\"📦 HandoffInputFilter triggered!\")\n",
        "    print(\"🔹 Original input_history:\", data.input_history)\n",
        "\n",
        "    new_input = data.input_history\n",
        "    return HandoffInputData(\n",
        "        input_history=(new_input),\n",
        "        pre_handoff_items=data.pre_handoff_items,\n",
        "        new_items=data.new_items,\n",
        "    )\n",
        "\n",
        "class CustomModelProvider(ModelProvider):\n",
        "  def get_model(self,model_name)->Model:\n",
        "    return OpenAIChatCompletionsModel(model = model_name, openai_client=external_client)\n",
        "\n",
        "result_with_run_config_2 = await Runner.run(\n",
        "    starting_agent = agent_with_run_config_2,\n",
        "    input = \"Write an essay about 'Meta AI'\",\n",
        "    run_config= RunConfig(\n",
        "        model = \"gemini-2.5-flash\",\n",
        "        model_provider = CustomModelProvider(),\n",
        "        model_settings=ModelSettings(\n",
        "            temperature = 0.1,\n",
        "            max_tokens = 1000\n",
        "        ),\n",
        "        handoff_input_filter = my_handoff_filter,\n",
        "    )\n",
        ")\n",
        "print(result_with_run_config_2.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCwtE-PtZY3d",
        "outputId": "53c67092-6c2e-44b5-f025-ec28ea3edda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 HandoffInputFilter triggered!\n",
            "🔹 Original input_history: Write an essay about 'Meta AI'\n",
            "Meta AI powers intelligent features across Facebook, Instagram, and WhatsApp, enhancing user experiences with personalized content and creative tools. It also drives cutting-edge research in large language models and computer vision, shaping AI's future and the metaverse. This ongoing development aims to create more intuitive and immersive digital interactions for billions worldwide.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# input_guardrails"
      ],
      "metadata": {
        "id": "0ki6hU9N8VGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,RunConfig,ModelSettings,ModelProvider,OpenAIChatCompletionsModel,Model,GuardrailFunctionOutput,InputGuardrailTripwireTriggered,HandoffInputData,InputGuardrail,RunContextWrapper\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Filter(BaseModel):\n",
        "  reasoning : str\n",
        "  is_essay_asked : bool\n",
        "\n",
        "check_essay_gaurdrail_agent = Agent(\n",
        "     name = \" Guardrail Agent\",\n",
        "     instructions=\"Check whether the user asked about writing an essay\",\n",
        "     output_type= Filter,\n",
        "     model = \"gemini-2.5-flash\"\n",
        " )\n",
        "\n",
        "def input_quardrail(ctx: RunContextWrapper,agent:Agent, input_data)->GuardrailFunctionOutput:\n",
        "  print(\"Input Guardrail Working\")\n",
        "  result = Runner.run_sync(\n",
        "      check_essay_gaurdrail_agent,\n",
        "      input = input_data,\n",
        "      context = ctx.context\n",
        "  )\n",
        "  final_result = result.final_output_as(Filter)\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info = final_result,\n",
        "      tripwire_triggered = not final_result.is_essay_asked\n",
        "  )\n",
        "\n",
        "agent_with_run_config_handoff = Agent(\n",
        "    name = \"Essay Writer Agent\",\n",
        "    instructions=\"Write a 50 words essay for the topic given to you\",\n",
        ")\n",
        "\n",
        "agent_with_run_config_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    handoffs=[agent_with_run_config_handoff]\n",
        ")\n",
        "\n",
        "def my_handoff_filter(data: HandoffInputData) -> HandoffInputData:\n",
        "    print(\"📦 HandoffInputFilter triggered!\")\n",
        "    print(\"🔹 Original input_history:\", data.input_history)\n",
        "\n",
        "    new_input = data.input_history\n",
        "    return HandoffInputData(\n",
        "        input_history=(new_input),\n",
        "        pre_handoff_items=data.pre_handoff_items,\n",
        "        new_items=data.new_items,\n",
        "    )\n",
        "\n",
        "class CustomModelProvider(ModelProvider):\n",
        "  def get_model(self,model_name)->Model:\n",
        "    return OpenAIChatCompletionsModel(model = model_name, openai_client=external_client)\n",
        "\n",
        "try :\n",
        "  result_with_run_config_2 = await Runner.run(\n",
        "      starting_agent = agent_with_run_config_2,\n",
        "      input = \"What is the capital of Italy\",\n",
        "      run_config= RunConfig(\n",
        "          model = \"gemini-2.5-flash\",\n",
        "          model_provider = CustomModelProvider(),\n",
        "          model_settings=ModelSettings(\n",
        "              temperature = 0.1,\n",
        "              max_tokens = 1000\n",
        "          ),\n",
        "          handoff_input_filter = my_handoff_filter,\n",
        "          input_guardrails = [InputGuardrail(\n",
        "              guardrail_function = input_quardrail\n",
        "          )]\n",
        "\n",
        "      )\n",
        "  )\n",
        "except InputGuardrailTripwireTriggered as e:\n",
        "  print(e)\n",
        "  print(e.guardrail_result.output.output_info.reasoning)\n",
        "  print(e.guardrail_result.output.output_info.is_essay_asked)\n",
        "\n",
        "print(result_with_run_config_2.final_output)"
      ],
      "metadata": {
        "id": "cK0tjLMkmq9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfd8b5e-2740-424e-91a1-2090b183c8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Guardrail Working\n",
            "Guardrail InputGuardrail triggered tripwire\n",
            "The user is asking a factual question about the capital of Italy, not about writing an essay.\n",
            "False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output_guradrails"
      ],
      "metadata": {
        "id": "k0KZUcoM8QFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import final\n",
        "from agents import Agent,Runner,RunConfig,ModelSettings,ModelProvider,OpenAIChatCompletionsModel,Model,GuardrailFunctionOutput,OutputGuardrail,OutputGuardrailTripwireTriggered,InputGuardrailTripwireTriggered,HandoffInputData,InputGuardrail,RunContextWrapper\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Filter(BaseModel):\n",
        "  reasoning : str\n",
        "  is_essay_asked : bool\n",
        "\n",
        "check_essay_gaurdrail_agent = Agent(\n",
        "     name = \" Guardrail Agent\",\n",
        "     instructions=\"Check whether the user asked about writing an essay\",\n",
        "     output_type= Filter,\n",
        "     model = \"gemini-2.5-flash\"\n",
        " )\n",
        "\n",
        "def input_quardrail(ctx: RunContextWrapper,agent:Agent, input_data)->GuardrailFunctionOutput:\n",
        "  print(\"Input Guardrail Working\")\n",
        "  result = Runner.run_sync(\n",
        "      check_essay_gaurdrail_agent,\n",
        "      input = input_data,\n",
        "      context = ctx.context\n",
        "  )\n",
        "  final_result = result.final_output_as(Filter)\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info = final_result,\n",
        "      tripwire_triggered = not final_result.is_essay_asked\n",
        "  )\n",
        "\n",
        "class Output_Filter(BaseModel):\n",
        "  response : str\n",
        "  is_response_contains_name : bool\n",
        "\n",
        "check_name_output_guardrail_agent = Agent(\n",
        "    name = \"Output Guardrail Agent\",\n",
        "    instructions=\"Check whether the output contains the name\",\n",
        "    output_type= Output_Filter,\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "def output_quardrail(ctx: RunContextWrapper,agent:Agent, input_data)->GuardrailFunctionOutput:\n",
        "  print(\"Output Guardrail Working\")\n",
        "  result = Runner.run_sync(\n",
        "      check_name_output_guardrail_agent,\n",
        "      input = input_data,\n",
        "      context = ctx.context\n",
        "  )\n",
        "  final_result = result.final_output_as(Output_Filter)\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info = final_result,\n",
        "      tripwire_triggered = final_result.is_response_contains_name\n",
        "  )\n",
        "\n",
        "agent_with_run_config_handoff = Agent(\n",
        "    name = \"Essay Writer Agent\",\n",
        "    instructions=\"Write a 50 words essay for the topic given to you\",\n",
        ")\n",
        "\n",
        "agent_with_run_config_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    handoffs=[agent_with_run_config_handoff]\n",
        ")\n",
        "\n",
        "def my_handoff_filter(data: HandoffInputData) -> HandoffInputData:\n",
        "    print(\"📦 HandoffInputFilter triggered!\")\n",
        "    print(\"🔹 Original input_history:\", data.input_history)\n",
        "\n",
        "    new_input = data.input_history\n",
        "    return HandoffInputData(\n",
        "        input_history=(new_input),\n",
        "        pre_handoff_items=data.pre_handoff_items,\n",
        "        new_items=data.new_items,\n",
        "    )\n",
        "\n",
        "class CustomModelProvider(ModelProvider):\n",
        "  def get_model(self,model_name)->Model:\n",
        "    return OpenAIChatCompletionsModel(model = model_name, openai_client=external_client)\n",
        "\n",
        "try :\n",
        "  result_with_run_config_2 = await Runner.run(\n",
        "      starting_agent = agent_with_run_config_2,\n",
        "      input = \"Write an essay about me with my name, I am an AI Engineer and my name is 'Saim'\",\n",
        "      run_config= RunConfig(\n",
        "          model = \"gemini-2.5-flash\",\n",
        "          model_provider = CustomModelProvider(),\n",
        "          model_settings=ModelSettings(\n",
        "              temperature = 0.1,\n",
        "              max_tokens = 1000\n",
        "          ),\n",
        "          handoff_input_filter = my_handoff_filter,\n",
        "          input_guardrails = [InputGuardrail(\n",
        "              guardrail_function = input_quardrail\n",
        "          )],\n",
        "          output_guardrails = [OutputGuardrail(\n",
        "              guardrail_function = output_quardrail\n",
        "          )]\n",
        "\n",
        "      )\n",
        "  )\n",
        "except (InputGuardrailTripwireTriggered,OutputGuardrailTripwireTriggered) as e:\n",
        "  print(e)\n",
        "  print(e.guardrail_result.output.output_info)\n",
        "  print(e.guardrail_result.output.tripwire_triggered)\n",
        "\n",
        "print(result_with_run_config_2.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDkoNbfa3Xn2",
        "outputId": "4f0a4f15-4e67-4605-f987-7a8637b6e633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Guardrail Working\n",
            "📦 HandoffInputFilter triggered!\n",
            "🔹 Original input_history: Write an essay about me with my name, I am an AI Engineer and my name is 'Saim'\n",
            "Output Guardrail Working\n",
            "Guardrail OutputGuardrail triggered tripwire\n",
            "response=\"Saim, an AI Engineer, is a visionary architect of the future. He meticulously designs and implements intelligent systems, transforming complex data into innovative solutions. His expertise empowers machines to learn, think, and assist, pushing the boundaries of what's possible. Saim's dedication to artificial intelligence isn't just a career; it's a passion for building a smarter, more connected world.\" is_response_contains_name=True\n",
            "True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Parameters Include :\n",
        "\n",
        "**1. tracing_disabled**\n",
        "\n",
        "**2. trace_id**\n",
        "\n",
        "**3. group_id**\n",
        "\n",
        "**4. workflow_name**\n",
        "\n",
        "**5. tracing_include_sensitive_data**\n",
        "\n",
        "**6. tracing_metadata**"
      ],
      "metadata": {
        "id": "W8BBtwWp9EZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,RunConfig\n",
        "\n",
        "agent_with_run_config_3 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=\"General purpose Assistant that answer the user queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "result = await Runner.run(\n",
        "    starting_agent = agent_with_run_config_3,\n",
        "    input = \"who is the founder of Anthropic\",\n",
        "    run_config=RunConfig(\n",
        "        tracing_disabled=True,   # tracing_disabled means no tracing is donwe for the agent run like no logging\n",
        "        trace_include_sensitive_data=False,   # It means that if tracing enabled it does not include any sensitive data from context or tool calls\n",
        "        workflow_name=\"Agentic WorkFlow\",   # name of our workflow for better understanding and distinguishing from other workflows\n",
        "        trace_id=\"my_trace-786\",             # unique trace_id for every trave if not given SDk creates by it self\n",
        "        group_id=\"my_chat_group_thread_786\",  # to connect multiple traces togather like a thread\n",
        "        trace_metadata={\n",
        "            \"my_key\": \"my_value\"             # additional data to include in our tracing\n",
        "        }\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "bXnQ_VAe7iN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}