{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd4JoNLaiOeNn1efc0o8/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saim-Hassan786/Learn-Agentic-AI-With-OpenAI-Agents-SDK/blob/main/07-Tracing/Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing\n",
        "Tracing is an inbuilt feature iven by the OpenAI Agents SDK that is used for logging and debuging of entier agent run , tracing is divided into to types:\n",
        "\n",
        "1. **Trace**: this is a top-level recoed of whole agent workflow or run\n",
        "2. **Span** : this includes a timed step in a trace that logs a single specific operation like LLM response, tool call and handoffs etc.  "
      ],
      "metadata": {
        "id": "oxSB_KJOt-DN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhrHYsAdzNny",
        "outputId": "7494f85a-ccd5-4ea4-f21e-1cee16d686b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m757.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the SDK\n",
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running event loop\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "J9rYqXSSzYEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre requisites SetUp\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY= userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from agents import set_default_openai_api,set_default_openai_client,\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "set_default_openai_client(external_client)\n",
        "set_default_openai_api(\"chat_completions\")"
      ],
      "metadata": {
        "id": "JaFph2tAzaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Tracing Setup\n",
        "This is the most simple one and this tracing can be done by using only OpenAI API Keys and can be traced on OpenAI Platform only and these tracings are exported there.\n",
        "To enable this , made following changes in above cell code:\n",
        "\n",
        "1. **Replace Gemini API Key With OpenAI API Key**\n",
        "2. **Replace Gemini Model With OpenAI Model**"
      ],
      "metadata": {
        "id": "Q6NSdLL_vwji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,set_tracing_disabled,trace\n",
        "set_tracing_disabled(False)\n",
        "\n",
        "agent_with_tracing = Agent(\n",
        "    name=\"Tracing Agent\",\n",
        "    instructions=\"A simple assistant agent\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "# tracing setup\n",
        "with trace(\"Simple Tracing\"):\n",
        "  result_with_tracing = await Runner.run(\n",
        "    agent_with_tracing,\n",
        "    \"Hi there , myself Saim?\"\n",
        ")\n",
        "  print(result_with_tracing.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDdDXc73tSW0",
        "outputId": "7a859658-0fe2-4268-8eb0-d34024e88a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Saim! Nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing to view Locally can also be called a Local Tracing"
      ],
      "metadata": {
        "id": "KMKoR5LKyiPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import local\n",
        "from agents import Agent,Runner,set_trace_processors,set_default_openai_client,set_default_openai_api,set_trace_processors,trace\n",
        "from openai import AsyncOpenAI\n",
        "from agents.tracing.processor_interface import TracingProcessor\n",
        "\n",
        "external_client_2 =  AsyncOpenAI(\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "set_default_openai_client(\n",
        "    client=external_client_2,\n",
        "    use_for_tracing=True\n",
        ")\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "# tracing\n",
        "class LocalTracing(TracingProcessor):\n",
        "  def __init__(self):\n",
        "    self.traces = []\n",
        "    self.spans = []\n",
        "\n",
        "  def on_trace_start(self, trace):\n",
        "    self.traces.append(trace)\n",
        "    print(\"========Trace Started========\")\n",
        "    print(f\"Trace Name = {trace.name}\")\n",
        "\n",
        "  def on_trace_end(self, trace):\n",
        "    print(\"========Trace Ended========\")\n",
        "    print(f\"Trace Name = {trace.name}\")\n",
        "    print(\"******TRACE ENDED******\")\n",
        "    print(\"======TRACE DETAILS======\")\n",
        "    print(f\"{trace.export()}\")\n",
        "\n",
        "  def on_span_start(self, span) :\n",
        "    self.spans.append(span)\n",
        "    print(\"========Span Started========\")\n",
        "    print(f\"Span Id = {span.span_id}\")\n",
        "    print(\"======SPAN DETAILS======\")\n",
        "    print(span.export())\n",
        "\n",
        "  def on_span_end(self, span):\n",
        "    print(\"========Span Ended========\")\n",
        "    print(f\"Span Id = {span.span_id}\")\n",
        "    print(\"******SPAN ENDED******\")\n",
        "    print(\"======SPAN DETAILS======\")\n",
        "    print(span.export())\n",
        "\n",
        "  def force_flush(self) :\n",
        "   pass    # use this to flush tracing into some data file etc , have to implement thi here as it is an abstract method of TracinProcessor Cls\n",
        "\n",
        "  def shutdown(self):\n",
        "    print(\"=======SHUTTING DOWN LOCAL TRACING========\")\n",
        "    print(\"*************Collected Traces****************\")\n",
        "    for trace in self.traces:\n",
        "      print(trace.export())\n",
        "    print(\"*************Collected Spans****************\")\n",
        "    for span in self.spans:\n",
        "      print(span.export())\n",
        "\n",
        "# local tracing setup\n",
        "local_tracing_processor = LocalTracing()\n",
        "set_trace_processors([local_tracing_processor])\n",
        "\n",
        "# now agent setup\n",
        "\n",
        "agent_with_local_tracing = Agent(\n",
        "    name = \"Local Tracing Agent\",\n",
        "    instructions = \"A simple assistant agent\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "with trace(\"Local Tracing\"):\n",
        "  result_with_local_tracing = await Runner.run(\n",
        "    agent_with_local_tracing,\n",
        "    \"Hi there , myself Saim?\"\n",
        "  )\n",
        "  print(\"************Final Output*************\")\n",
        "  print(result_with_local_tracing.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2giCIpJCxyQC",
        "outputId": "719a4330-8c73-4ec5-e791-4ff02a922171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========Trace Started========\n",
            "Trace Name = Local Tracing\n",
            "========Span Started========\n",
            "Span Id = span_c4dbae70b37948d78fadcbd7\n",
            "======SPAN DETAILS======\n",
            "{'object': 'trace.span', 'id': 'span_c4dbae70b37948d78fadcbd7', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': None, 'started_at': '2025-07-09T11:27:11.418253+00:00', 'ended_at': None, 'span_data': {'type': 'agent', 'name': 'Local Tracing Agent', 'handoffs': [], 'tools': None, 'output_type': 'str'}, 'error': None}\n",
            "========Span Started========\n",
            "Span Id = span_cee28fd9fa3d4929a53b3ff8\n",
            "======SPAN DETAILS======\n",
            "{'object': 'trace.span', 'id': 'span_cee28fd9fa3d4929a53b3ff8', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': 'span_c4dbae70b37948d78fadcbd7', 'started_at': '2025-07-09T11:27:11.419039+00:00', 'ended_at': None, 'span_data': {'type': 'generation', 'input': None, 'output': None, 'model': 'gemini-2.5-flash', 'model_config': {'temperature': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None, 'tool_choice': None, 'parallel_tool_calls': None, 'truncation': None, 'max_tokens': None, 'reasoning': None, 'metadata': None, 'store': None, 'include_usage': None, 'response_include': None, 'extra_query': None, 'extra_body': None, 'extra_headers': None, 'extra_args': None, 'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/'}, 'usage': None}, 'error': None}\n",
            "========Span Ended========\n",
            "Span Id = span_cee28fd9fa3d4929a53b3ff8\n",
            "******SPAN ENDED******\n",
            "======SPAN DETAILS======\n",
            "{'object': 'trace.span', 'id': 'span_cee28fd9fa3d4929a53b3ff8', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': 'span_c4dbae70b37948d78fadcbd7', 'started_at': '2025-07-09T11:27:11.419039+00:00', 'ended_at': '2025-07-09T11:27:12.158486+00:00', 'span_data': {'type': 'generation', 'input': [{'content': 'A simple assistant agent', 'role': 'system'}, {'role': 'user', 'content': 'Hi there , myself Saim?'}], 'output': [{'content': 'Hi Saim! Nice to meet you. How can I help you today?', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': None}], 'model': 'gemini-2.5-flash', 'model_config': {'temperature': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None, 'tool_choice': None, 'parallel_tool_calls': None, 'truncation': None, 'max_tokens': None, 'reasoning': None, 'metadata': None, 'store': None, 'include_usage': None, 'response_include': None, 'extra_query': None, 'extra_body': None, 'extra_headers': None, 'extra_args': None, 'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/'}, 'usage': {'input_tokens': 13, 'output_tokens': 16}}, 'error': None}\n",
            "========Span Ended========\n",
            "Span Id = span_c4dbae70b37948d78fadcbd7\n",
            "******SPAN ENDED******\n",
            "======SPAN DETAILS======\n",
            "{'object': 'trace.span', 'id': 'span_c4dbae70b37948d78fadcbd7', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': None, 'started_at': '2025-07-09T11:27:11.418253+00:00', 'ended_at': '2025-07-09T11:27:12.159641+00:00', 'span_data': {'type': 'agent', 'name': 'Local Tracing Agent', 'handoffs': [], 'tools': [], 'output_type': 'str'}, 'error': None}\n",
            "************Final Output*************\n",
            "Hi Saim! Nice to meet you. How can I help you today?\n",
            "========Trace Ended========\n",
            "Trace Name = Local Tracing\n",
            "******TRACE ENDED******\n",
            "======TRACE DETAILS======\n",
            "{'object': 'trace', 'id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'workflow_name': 'Local Tracing', 'group_id': None, 'metadata': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To execute the shutdown() method in our local tracing processor , we have to shutdown our processor**"
      ],
      "metadata": {
        "id": "o2qW3R0m8QyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_tracing_processor.shutdown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dWtR0626NYz",
        "outputId": "c2b91e08-1b3c-4f0b-fa1a-e7f4802576a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======SHUTTING DOWN LOCAL TRACING========\n",
            "*************Collected Traces****************\n",
            "{'object': 'trace', 'id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'workflow_name': 'Local Tracing', 'group_id': None, 'metadata': None}\n",
            "*************Collected Spans****************\n",
            "{'object': 'trace.span', 'id': 'span_c4dbae70b37948d78fadcbd7', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': None, 'started_at': '2025-07-09T11:27:11.418253+00:00', 'ended_at': '2025-07-09T11:27:12.159641+00:00', 'span_data': {'type': 'agent', 'name': 'Local Tracing Agent', 'handoffs': [], 'tools': [], 'output_type': 'str'}, 'error': None}\n",
            "{'object': 'trace.span', 'id': 'span_cee28fd9fa3d4929a53b3ff8', 'trace_id': 'trace_64109261d0cf4ea3950368f8e672d2ff', 'parent_id': 'span_c4dbae70b37948d78fadcbd7', 'started_at': '2025-07-09T11:27:11.419039+00:00', 'ended_at': '2025-07-09T11:27:12.158486+00:00', 'span_data': {'type': 'generation', 'input': [{'content': 'A simple assistant agent', 'role': 'system'}, {'role': 'user', 'content': 'Hi there , myself Saim?'}], 'output': [{'content': 'Hi Saim! Nice to meet you. How can I help you today?', 'refusal': None, 'role': 'assistant', 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': None}], 'model': 'gemini-2.5-flash', 'model_config': {'temperature': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None, 'tool_choice': None, 'parallel_tool_calls': None, 'truncation': None, 'max_tokens': None, 'reasoning': None, 'metadata': None, 'store': None, 'include_usage': None, 'response_include': None, 'extra_query': None, 'extra_body': None, 'extra_headers': None, 'extra_args': None, 'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/'}, 'usage': {'input_tokens': 13, 'output_tokens': 16}}, 'error': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing With 3rd Party Tracing Processor"
      ],
      "metadata": {
        "id": "sjKNuVoI-cFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can connect different 3rd party tracing processors to catch , save and display our traces of the Agent Run for us , these different tracing processors are :**\n",
        "\n",
        "1. **Braintrust**\n",
        "2. **Pydantic Logfire**\n",
        "3. **AgentOps**\n",
        "4. **Scorecard**\n",
        "5. **Keywords AI**\n",
        "6. **LangSmith**\n",
        "7. **Maxim AI**\n",
        "8. **Comet Opik**\n",
        "9. **Langfuse**\n",
        "10. **Langtrace**\n",
        "\n",
        "**And many more......**\n",
        "\n",
        "# **For example , I will implement AgentOps here below**"
      ],
      "metadata": {
        "id": "QUP8N-3oFp3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq agentops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LByB8YDM8Lv_",
        "outputId": "08006152-414e-4f67-ecc2-508ea1e504fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/279.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m279.1/279.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/65.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "AGENTOPS_API_KEY = userdata.get(\"AGENTOPS_API_KEY_2\")"
      ],
      "metadata": {
        "id": "brFtt1azHAgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import agentops\n",
        "agentops.init(AGENTOPS_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mn6yqA9HLIP",
        "outputId": "8154cb66-1526-4a04-9350-d4cd149fcc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ğŸ–‡ AgentOps: [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'\n",
            "ğŸ–‡ AgentOps: AgentOps: Successfully instrumented 'OpenaiInstrumentor' for package 'agentops.instrumentation.providers.openai'.\n",
            "ğŸ–‡ AgentOps: AgentOps: 'agents' is the first-targeted agentic library. Will uninstrument providers if any are/become active.\n",
            "ğŸ–‡ AgentOps: AgentOps: Uninstrumented provider: OpenaiInstrumentor (for package 'openai') due to agentic library activation.\n",
            "ğŸ–‡ AgentOps: [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'\n",
            "ğŸ–‡ AgentOps: AgentOps: Successfully instrumented 'OpenaiInstrumentor' for package 'agentops.instrumentation.providers.openai'.\n",
            "ğŸ–‡ AgentOps: AgentOps: Successfully instrumented 'OpenAIAgentsInstrumentor' for package 'agentops.instrumentation.agentic.openai_agents'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "async def main():\n",
        "   agent = Agent(\n",
        "    name = \"Joke Teller Agent\",\n",
        "    instructions=\"Tell a joke to the user\",\n",
        "    model = \"gemini-2.0-flash\"\n",
        ")\n",
        "\n",
        "   with trace(\"Joker Tracing\"):\n",
        "    result_1 = await Runner.run(agent, \"Tell me a joke\")\n",
        "    result_2 = await Runner.run(agent, f\"Rate this joke out of 10 {result_1.final_output}\")\n",
        "    print(result_1.final_output)\n",
        "    print(result_2.final_output)\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2mx8SWBHd_y",
        "outputId": "680f5ae8-bad6-459a-ee3a-81172e9cf554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "Okay, here's my rating and why:\n",
            "\n",
            "*   **Rating: 7/10**\n",
            "\n",
            "Here's my reasoning:\n",
            "\n",
            "*   **The Setup:** The setup is classic and familiar. It poses a question that makes you think there might be a complicated or science-specific reason.\n",
            "*   **The Punchline:** The punchline is simple, punny, and relatable. Everyone understands that atoms are the building blocks of matter, and the play on words is effective.\n",
            "*   **Overall:** It's a clean, easy-to-understand joke that gets a chuckle. It's not mind-blowing, but it's solid and works well as a quick, lighthearted joke.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All the Traces and Spans are sent directly to the AgentOps DashBoard and can be Observed from there .**"
      ],
      "metadata": {
        "id": "tsYwXny5IJ3k"
      }
    }
  ]
}
