{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEHjR9+qRxA1fO9OGYwWpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saim-Hassan786/Learn-Agentic-AI/blob/main/03-Results-Returned-From-Runner-Loop/Results_Returned_From_Runner_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "Result represents the return results of the Agent Loop which can either be **RunResult** and **RunResultStreaming**"
      ],
      "metadata": {
        "id": "pmqwbu8bzejn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhrHYsAdzNny",
        "outputId": "7e4789dc-e95b-4370-ab81-c26ec91fca48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the SDK\n",
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running event loop\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "J9rYqXSSzYEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre requisites SetUp\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY= userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from agents import set_default_openai_api,set_default_openai_client,set_tracing_disabled\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "set_default_openai_client(external_client)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "set_tracing_disabled(True)"
      ],
      "metadata": {
        "id": "JaFph2tAzaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunResultBase\n",
        "RunResultBase is the base class for all the Results Both **RunResult** And **RunResultStreaming** .It contains the following methods and attributes :\n",
        "\n",
        "1. **inputs**\n",
        "2. **new_items**\n",
        "3. **raw_responses**\n",
        "4. **final_output**\n",
        "5. **input_guardrail_results**\n",
        "6. **output_guardrail_results**\n",
        "7. **context_wrapper**\n",
        "7. **last_agent()**\n",
        "8. **final_output_as()**\n",
        "9. **to_input_list()**\n",
        "10. **last_response_id()**"
      ],
      "metadata": {
        "id": "irWlKSe30xg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunResult"
      ],
      "metadata": {
        "id": "MMgFt-ZOD-x-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhTpPh7ozWDQ"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class OutputType(BaseModel):\n",
        "  response : str\n",
        "  is_homework : bool\n",
        "\n",
        "agent = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions = \"Reply with user queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "result = await Runner.run(\n",
        "    agent,\n",
        "    \"What is the defination of Artificial Neural Networks in 2 lines\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.input)\n",
        "print(\"===========\"*20)\n",
        "print(result.new_items)\n",
        "print(\"===========\"*20)\n",
        "print(result.raw_responses)\n",
        "print(\"===========\"*20)\n",
        "print(result.final_output)\n",
        "print(\"===========\"*20)\n",
        "print(result.input_guardrail_results)\n",
        "print(\"===========\"*20)\n",
        "print(result.output_guardrail_results)\n",
        "print(\"===========\"*20)\n",
        "print(result.context_wrapper)\n",
        "print(\"===========\"*20)\n",
        "print(result.last_agent)\n",
        "print(\"===========\"*20)\n",
        "print(result.final_output_as(OutputType))\n",
        "print(\"===========\"*20)\n",
        "print(result.to_input_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRvFjAhk43vs",
        "outputId": "43289877-0c9e-48e9-8542-c56595c576fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the defination of Artificial Neural Networks in 2 lines\n",
            "============================================================================================================================================================================================================================\n",
            "[MessageOutputItem(agent=Agent(name='Assistant Agent', instructions='Reply with user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Artificial Neural Networks (ANNs) are computational models inspired by the human brain, composed of interconnected nodes (neurons) organized in layers.\\nThey learn from data to identify complex patterns and relationships, enabling tasks like prediction, classification, and decision-making.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n",
            "============================================================================================================================================================================================================================\n",
            "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Artificial Neural Networks (ANNs) are computational models inspired by the human brain, composed of interconnected nodes (neurons) organized in layers.\\nThey learn from data to identify complex patterns and relationships, enabling tasks like prediction, classification, and decision-making.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=19, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=51, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=893), response_id=None)]\n",
            "============================================================================================================================================================================================================================\n",
            "Artificial Neural Networks (ANNs) are computational models inspired by the human brain, composed of interconnected nodes (neurons) organized in layers.\n",
            "They learn from data to identify complex patterns and relationships, enabling tasks like prediction, classification, and decision-making.\n",
            "============================================================================================================================================================================================================================\n",
            "[]\n",
            "============================================================================================================================================================================================================================\n",
            "[]\n",
            "============================================================================================================================================================================================================================\n",
            "RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=19, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=51, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=893))\n",
            "============================================================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='Reply with user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "============================================================================================================================================================================================================================\n",
            "Artificial Neural Networks (ANNs) are computational models inspired by the human brain, composed of interconnected nodes (neurons) organized in layers.\n",
            "They learn from data to identify complex patterns and relationships, enabling tasks like prediction, classification, and decision-making.\n",
            "============================================================================================================================================================================================================================\n",
            "[{'content': 'What is the defination of Artificial Neural Networks in 2 lines', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': 'Artificial Neural Networks (ANNs) are computational models inspired by the human brain, composed of interconnected nodes (neurons) organized in layers.\\nThey learn from data to identify complex patterns and relationships, enabling tasks like prediction, classification, and decision-making.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunResultStreaming"
      ],
      "metadata": {
        "id": "7Iaou-ghEB4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class OutputType_2(BaseModel):\n",
        "  response : str\n",
        "  is_homework : bool\n",
        "\n",
        "agent_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions = \"Reply with user queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "result_2 = Runner.run_streamed(\n",
        "    agent_2,\n",
        "    \"What is the defination of Generative AI in 2 lines\"\n",
        ")"
      ],
      "metadata": {
        "id": "b7MXjbnSD035"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_2.input)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.new_items)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.raw_responses)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.final_output)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.input_guardrail_results)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.output_guardrail_results)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.context_wrapper)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.last_agent)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.final_output_as(OutputType_2))\n",
        "print(\"=========\"*20)\n",
        "print(result_2.to_input_list())\n",
        "print(\"=========\"*20)\n",
        "print(result_2.last_response_id)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.is_complete)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.current_agent)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.current_turn)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.context_wrapper)\n",
        "print(\"=========\"*20)\n",
        "print(result_2.stream_events())\n",
        "print(\"=========\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobmYbmQEUi2",
        "outputId": "5279e0bc-ab1f-47cd-c070-f93ac80d83cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the defination of Generative AI in 2 lines\n",
            "====================================================================================================================================================================================\n",
            "[MessageOutputItem(agent=Agent(name='Assistant Agent', instructions='Reply with user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Generative AI is a type of artificial intelligence that can create new, original content, such as text, images, or audio. It achieves this by learning patterns and structures from vast amounts of existing data, allowing it to generate unique outputs that resemble the training data but are not direct copies.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n",
            "====================================================================================================================================================================================\n",
            "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Generative AI is a type of artificial intelligence that can create new, original content, such as text, images, or audio. It achieves this by learning patterns and structures from vast amounts of existing data, allowing it to generate unique outputs that resemble the training data but are not direct copies.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0), response_id='__fake_id__')]\n",
            "====================================================================================================================================================================================\n",
            "Generative AI is a type of artificial intelligence that can create new, original content, such as text, images, or audio. It achieves this by learning patterns and structures from vast amounts of existing data, allowing it to generate unique outputs that resemble the training data but are not direct copies.\n",
            "====================================================================================================================================================================================\n",
            "[]\n",
            "====================================================================================================================================================================================\n",
            "[]\n",
            "====================================================================================================================================================================================\n",
            "RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "====================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='Reply with user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "====================================================================================================================================================================================\n",
            "Generative AI is a type of artificial intelligence that can create new, original content, such as text, images, or audio. It achieves this by learning patterns and structures from vast amounts of existing data, allowing it to generate unique outputs that resemble the training data but are not direct copies.\n",
            "====================================================================================================================================================================================\n",
            "[{'content': 'What is the defination of Generative AI in 2 lines', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': 'Generative AI is a type of artificial intelligence that can create new, original content, such as text, images, or audio. It achieves this by learning patterns and structures from vast amounts of existing data, allowing it to generate unique outputs that resemble the training data but are not direct copies.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}]\n",
            "====================================================================================================================================================================================\n",
            "__fake_id__\n",
            "====================================================================================================================================================================================\n",
            "True\n",
            "====================================================================================================================================================================================\n",
            "Agent(name='Assistant Agent', instructions='Reply with user queries', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "====================================================================================================================================================================================\n",
            "1\n",
            "====================================================================================================================================================================================\n",
            "RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "====================================================================================================================================================================================\n",
            "<async_generator object RunResultStreaming.stream_events at 0x7edb123e5b60>\n",
            "====================================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming With stream_events()"
      ],
      "metadata": {
        "id": "y78SrrIlUk1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all events\n",
        "import asyncio\n",
        "async def main_1():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=\"gemini-2.5-flash\",\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "    async for event in result.stream_events():\n",
        "        print(event)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main_1())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCdafml9RBE0",
        "outputId": "f6c0c412-05b9-466b-ce62-c4d3dc3ce6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1751630321.6055677, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Here are 5 jokes for you:\\n\\n1.  Why don', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'t scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  What do you call a fake noodle?\\n    An impasta!\\n\\n3.  Why did the scarecrow win an award?\\n    Because he\", item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" was outstanding in his field!\\n\\n4.  What do you call cheese that isn't yours?\\n    Nacho cheese!\\n\\n5.  Why was the math book sad?\\n    Because it had too many problems.\", item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  What do you call a fake noodle?\\n    An impasta!\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  What do you call cheese that isn't yours?\\n    Nacho cheese!\\n\\n5.  Why was the math book sad?\\n    Because it had too many problems.\", type='output_text', logprobs=None), sequence_number=6, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  What do you call a fake noodle?\\n    An impasta!\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  What do you call cheese that isn't yours?\\n    Nacho cheese!\\n\\n5.  Why was the math book sad?\\n    Because it had too many problems.\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=7, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1751630321.6055677, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  What do you call a fake noodle?\\n    An impasta!\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  What do you call cheese that isn't yours?\\n    Nacho cheese!\\n\\n5.  Why was the math book sad?\\n    Because it had too many problems.\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=8, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  What do you call a fake noodle?\\n    An impasta!\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  What do you call cheese that isn't yours?\\n    Nacho cheese!\\n\\n5.  Why was the math book sad?\\n    Because it had too many problems.\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final_output from stream_events\n",
        "# agent_updated_event from stream_events\n",
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main_2():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=\"gemini-2.5-flash\",\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Tell me 5 jokes'\")\n",
        "    async for event in result.stream_events():\n",
        "        if event.type == \"raw_response_event\" and isinstance(event.data , ResponseTextDeltaEvent):\n",
        "            print(event.data.delta, end=\"\", flush=True,)\n",
        "        if event.type == \"agent_updated_stream_event\":\n",
        "            print(event, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main_2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHhCyJGRH9Vp",
        "outputId": "10e90b92-4291-48d3-d1c6-4395591787f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')Here are 5 jokes for you!\n",
            "\n",
            "1.  Why don't scientists trust atoms?\n",
            "    *   Because they make up everything!\n",
            "\n",
            "2.  What do you call a fish with no eyes?\n",
            "    *   Fsh!\n",
            "\n",
            "3.  Why did the scarecrow win an award?\n",
            "    *   Because he was outstanding in his field!\n",
            "\n",
            "4.  What's orange and sounds like a parrot?\n",
            "    *   A carrot!\n",
            "\n",
            "5.  Why did the bicycle fall over?\n",
            "    *   Because it was two-tired!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RunItem containing Handoff and Final Output from stream_events\n",
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent, Runner\n",
        "\n",
        "agent_handoff = Agent(\n",
        "    name = \"Summary Agent\",\n",
        "    instructions = \"Generate a summary for the topic\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "\n",
        "async def main_3():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        handoffs = [agent_handoff]\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Gererate a summary for the topic 'AI will be everywhere' in 100 words\")\n",
        "    async for event in result.stream_events():\n",
        "        print(event)\n",
        "        print(\"=========\"*20)\n",
        "        if event.type == \"run_item_stream_event\":\n",
        "            print(event, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main_3())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04526644-7b6d-4eeb-9d9a-4e7ded10e898",
        "id": "yZOlpsoeVA58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1751633014.551979, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='transfer_to_summary_agent', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='transfer_to_summary_agent', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1751633014.551979, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='transfer_to_summary_agent', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RunItemStreamEvent(name='handoff_requested', item=HandoffCallItem(agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='transfer_to_summary_agent', type='function_call', id='__fake_id__', status=None), type='handoff_call_item'), type='run_item_stream_event')\n",
            "====================================================================================================================================================================================\n",
            "RunItemStreamEvent(name='handoff_requested', item=HandoffCallItem(agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='transfer_to_summary_agent', type='function_call', id='__fake_id__', status=None), type='handoff_call_item'), type='run_item_stream_event')RunItemStreamEvent(name='handoff_occured', item=HandoffOutputItem(agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '{\"assistant\": \"Summary Agent\"}', 'type': 'function_call_output'}, source_agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), target_agent=Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='handoff_output_item'), type='run_item_stream_event')\n",
            "====================================================================================================================================================================================\n",
            "RunItemStreamEvent(name='handoff_occured', item=HandoffOutputItem(agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '{\"assistant\": \"Summary Agent\"}', 'type': 'function_call_output'}, source_agent=Agent(name='Joker', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), target_agent=Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='handoff_output_item'), type='run_item_stream_event')AgentUpdatedStreamEvent(new_agent=Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1751633016.8282049, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" Artificial Intelligence into nearly every facet of our daily lives and industries. This isn't just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI's pervasive\", item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics', item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=', data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', item_id='__fake_id__', output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of Artificial Intelligence into nearly every facet of our daily lives and industries. This isn\\'t just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI\\'s pervasive presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics, data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', type='output_text', logprobs=None), sequence_number=7, type='response.content_part.done'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of Artificial Intelligence into nearly every facet of our daily lives and industries. This isn\\'t just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI\\'s pervasive presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics, data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1751633016.8282049, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of Artificial Intelligence into nearly every facet of our daily lives and industries. This isn\\'t just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI\\'s pervasive presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics, data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=9, type='response.completed'), type='raw_response_event')\n",
            "====================================================================================================================================================================================\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of Artificial Intelligence into nearly every facet of our daily lives and industries. This isn\\'t just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI\\'s pervasive presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics, data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n",
            "====================================================================================================================================================================================\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Summary Agent', instructions='Generate a summary for the topic', prompt=None, handoff_description=None, handoffs=[], model='gemini-2.5-flash', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The concept \"AI will be everywhere\" signifies the inevitable, widespread integration of Artificial Intelligence into nearly every facet of our daily lives and industries. This isn\\'t just about robots; it encompasses sophisticated algorithms powering our smartphones, smart homes, vehicles, healthcare diagnostics, financial systems, and city infrastructure. AI\\'s pervasive presence promises unprecedented levels of automation, personalization, and efficiency, transforming how we work, learn, commute, and interact with the world. While offering immense potential for progress and problem-solving, this omnipresence also raises crucial discussions around ethics, data privacy, job displacement, and the need for thoughtful development and regulation to ensure a beneficial future.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting All Stream Events Together"
      ],
      "metadata": {
        "id": "MiLWCgkOfyta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=\"gemini-2.5-flash\",\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "    )\n",
        "    print(\"=== Run starting ===\")\n",
        "\n",
        "    async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        # When the agent updates, print that\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        # When items are generated, print them\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "    print(\"=== Run complete ===\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENFPNmJFf29c",
        "outputId": "a7d786ec-d4ba-443f-9825-079d7950c2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run starting ===\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 3\n",
            "-- Message output:\n",
            " Here are 3 jokes:\n",
            "\n",
            "1. Why don't scientists trust atoms? Because they make up everything!\n",
            "2. What do you call a fake noodle? An impasta!\n",
            "3. Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "1. All the return results from the Runner Loop (RunResult or RunResultStreaming) have the same RunResultbase as Parent Class.\n",
        "2. For stream_events( ) that act as a yielder for the events created by the Runner loop one by one can have 3 Stream Events Types :\n",
        "   \n",
        "   - **AgentUpdatedStreamEvent** (agent updation)\n",
        "   - **RawResponseStreamEvent** (raw responses from the LLM)\n",
        "   - **RunItemStreamEvent** (wrap over items genrated by loop after the response from LLM either handoff, tool call, tool output etc)\n",
        "      \n",
        "       - **message_output_created**\n",
        "       - **tool_called**\n",
        "       - **tool_output**\n",
        "       - **handoff_called**\n",
        "       - **handoff_output**\n",
        "       - **reasonging_item_created**\n",
        "       - **mcp_approval_requested**\n",
        "       - **mcp_list_tools**"
      ],
      "metadata": {
        "id": "jw0HVujAinct"
      }
    }
  ]
}